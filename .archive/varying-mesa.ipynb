{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mesa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import norm\n",
    "from ipywidgets import interact, IntSlider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArmAgent(mesa.Agent):\n",
    "    \"\"\"An agent representing an arm in the multi-armed bandit problem.\"\"\"\n",
    "    \n",
    "    def __init__(self, unique_id, model, initial_mean, initial_std, variation_func):\n",
    "        super().__init__(unique_id, model)\n",
    "        self.initial_mean = initial_mean\n",
    "        self.initial_std = initial_std\n",
    "        self.variation_func = variation_func\n",
    "    \n",
    "    def get_mean(self):\n",
    "        \"\"\"Get the current mean based on the model's current step.\"\"\"\n",
    "        return self.variation_func(self.initial_mean, self.model.schedule.steps)\n",
    "    \n",
    "    def get_std(self):\n",
    "        \"\"\"Get the current standard deviation.\"\"\"\n",
    "        return self.initial_std\n",
    "    \n",
    "    def pull(self):\n",
    "        \"\"\"Pull the arm and return a reward.\"\"\"\n",
    "        return np.random.normal(self.get_mean(), self.get_std())\n",
    "\n",
    "def linear_variation(initial_mean, t):\n",
    "    return initial_mean + 0.01 * t\n",
    "\n",
    "def sinusoidal_variation(initial_mean, t):\n",
    "    return initial_mean + np.sin(t / 10) * 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BanditAgent(mesa.Agent):\n",
    "    \"\"\"An agent representing the bandit in the multi-armed bandit problem.\"\"\"\n",
    "    \n",
    "    def __init__(self, unique_id, model, strategy='epsilon_greedy', epsilon=0.1):\n",
    "        super().__init__(unique_id, model)\n",
    "        self.strategy = strategy\n",
    "        self.epsilon = epsilon\n",
    "        self.values = {arm.unique_id: 0 for arm in self.model.arms}\n",
    "        self.counts = {arm.unique_id: 0 for arm in self.model.arms}\n",
    "        self.cumulative_reward = 0\n",
    "        self.arm_selections = []\n",
    "    \n",
    "    def select_arm_random(self):\n",
    "        \"\"\"Select an arm randomly.\"\"\"\n",
    "        return self.random.choice(self.model.arms)\n",
    "    \n",
    "    def select_arm_greedy(self):\n",
    "        \"\"\"Select the arm with the highest estimated value.\"\"\"\n",
    "        return max(self.model.arms, key=lambda arm: self.values[arm.unique_id])\n",
    "    \n",
    "    def select_arm_epsilon_greedy(self):\n",
    "        \"\"\"Select an arm using epsilon-greedy strategy.\"\"\"\n",
    "        if self.random.random() < self.epsilon:\n",
    "            return self.select_arm_random()\n",
    "        else:\n",
    "            return self.select_arm_greedy()\n",
    "    \n",
    "    def select_arm(self):\n",
    "        \"\"\"Select an arm based on the current strategy.\"\"\"\n",
    "        if self.strategy == 'random':\n",
    "            return self.select_arm_random()\n",
    "        elif self.strategy == 'greedy':\n",
    "            return self.select_arm_greedy()\n",
    "        elif self.strategy == 'epsilon_greedy':\n",
    "            return self.select_arm_epsilon_greedy()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown strategy: {self.strategy}\")\n",
    "    \n",
    "    def update(self, chosen_arm, reward):\n",
    "        \"\"\"Update the estimated value of the chosen arm.\"\"\"\n",
    "        arm_id = chosen_arm.unique_id\n",
    "        self.counts[arm_id] += 1\n",
    "        n = self.counts[arm_id]\n",
    "        value = self.values[arm_id]\n",
    "        new_value = ((n - 1) / n) * value + (1 / n) * reward\n",
    "        self.values[arm_id] = new_value\n",
    "        self.cumulative_reward += reward\n",
    "    \n",
    "    def step(self):\n",
    "        \"\"\"Perform one step of the bandit algorithm.\"\"\"\n",
    "        chosen_arm = self.select_arm()\n",
    "        reward = chosen_arm.pull()\n",
    "        self.update(chosen_arm, reward)\n",
    "        self.arm_selections.append(chosen_arm.unique_id) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MABModel(mesa.Model):\n",
    "    \"\"\"A model for the multi-armed bandit problem.\"\"\"\n",
    "    \n",
    "    def __init__(self, arm_params, bandit_strategy='epsilon_greedy'):\n",
    "        self.num_arms = len(arm_params)\n",
    "        self.schedule = mesa.time.RandomActivation(self)\n",
    "        \n",
    "        # Create arms based on the provided parameters\n",
    "        self.arms = []\n",
    "        for i, (initial_mean, initial_std, variation_func) in enumerate(arm_params):\n",
    "            arm = ArmAgent(i, self, initial_mean, initial_std, variation_func)\n",
    "            self.arms.append(arm)\n",
    "            self.schedule.add(arm)\n",
    "        \n",
    "        # Create bandit\n",
    "        self.bandit = BanditAgent(self.num_arms, self, strategy=bandit_strategy)\n",
    "        self.schedule.add(self.bandit)\n",
    "        \n",
    "        self.datacollector = mesa.DataCollector(\n",
    "            model_reporters={\n",
    "                \"Cumulative Reward\": lambda m: m.bandit.cumulative_reward,\n",
    "                \"Strategy\": lambda m: m.bandit.strategy,\n",
    "                \"Arm Selections\": lambda m: m.bandit.arm_selections\n",
    "            },\n",
    "            agent_reporters={}\n",
    "        )\n",
    "    \n",
    "    def step(self):\n",
    "        \"\"\"Advance the model by one step.\"\"\"\n",
    "        self.schedule.step()\n",
    "        self.datacollector.collect(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_true_distributions(arms, max_time=1000):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    x = np.linspace(-5, 5, 1000)\n",
    "    colors = plt.cm.rainbow(np.linspace(0, 1, len(arms)))\n",
    "    \n",
    "    def update(time):\n",
    "        plt.clf()\n",
    "        for i, (arm, color) in enumerate(zip(arms, colors)):\n",
    "            mean = arm.variation_func(arm.initial_mean, time)\n",
    "            std = arm.get_std()\n",
    "            y = norm.pdf(x, mean, std)\n",
    "            plt.plot(x, y, label=f'Arm {i} (μ={mean:.2f}, σ={std:.2f})', color=color)\n",
    "            \n",
    "            # Add vertical line at the mean\n",
    "            plt.axvline(x=mean, color=color, linestyle='--', alpha=0.7)\n",
    "        \n",
    "        plt.title(f\"Reward Distributions for Each Arm (Time: {time})\")\n",
    "        plt.xlabel(\"Reward\")\n",
    "        plt.ylabel(\"Probability Density\")\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.ylim(0, 0.5)\n",
    "        plt.show()\n",
    "    \n",
    "    # Create an interactive slider\n",
    "    from ipywidgets import interact, IntSlider\n",
    "    time_slider = IntSlider(min=0, max=max_time, step=1, value=0, description='Time:')\n",
    "    interact(update, time=time_slider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create arms with different initial means, standard deviations, and variation functions\n",
    "arm_params = [\n",
    "    (0.0, 1, linear_variation),\n",
    "    (0.5, 1, sinusoidal_variation),\n",
    "    (1.0, 1, lambda m, t: m),\n",
    "    (1.5, 1, lambda m, t: m + 0.005 * t * np.sin(t / 5)),\n",
    "    (2.0, 1, lambda m, t: m - 0.02 * t)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the true reward distributions\n",
    "visualize_true_distributions([ArmAgent(i, None, *params) for i, params in enumerate(arm_params)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_arm_selection_frequencies(all_selections, arms, max_time=1000):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    colors = plt.cm.rainbow(np.linspace(0, 1, len(arms)))\n",
    "    \n",
    "    def update(time):\n",
    "        plt.clf()\n",
    "        selections_up_to_time = {strategy: selections[:time+1] for strategy, selections in all_selections.items()}\n",
    "        \n",
    "        x = np.arange(len(arms))\n",
    "        bar_width = 0.25\n",
    "        multiplier = 0\n",
    "\n",
    "        for strategy, selections in selections_up_to_time.items():\n",
    "            arm_pulls = [selections.count(arm.unique_id) for arm in arms]\n",
    "            offset = bar_width * multiplier\n",
    "            rects = plt.bar(x + offset, arm_pulls, bar_width, label=strategy)\n",
    "            plt.bar_label(rects, padding=3)\n",
    "            multiplier += 1\n",
    "\n",
    "        plt.title(f\"Arm Selection Frequencies (Time: {time})\")\n",
    "        plt.xlabel(\"Arm\")\n",
    "        plt.ylabel(\"Number of Pulls\")\n",
    "        plt.xticks(x + bar_width, [f\"Arm {i}\" for i in range(len(arms))])\n",
    "        plt.legend(loc='upper left')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Create an interactive slider\n",
    "    time_slider = IntSlider(min=0, max=max_time, step=1, value=0, description='Time:')\n",
    "    interact(update, time=time_slider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation parameters\n",
    "num_steps = 1000\n",
    "strategies = ['random', 'greedy', 'epsilon_greedy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run simulations for each strategy\n",
    "results = {}\n",
    "models = {}\n",
    "all_selections = {}\n",
    "\n",
    "for strategy in strategies:\n",
    "    model = MABModel(arm_params, bandit_strategy=strategy)\n",
    "    for _ in range(num_steps):\n",
    "        model.step()\n",
    "    \n",
    "    results[strategy] = model.datacollector.get_model_vars_dataframe()\n",
    "    models[strategy] = model\n",
    "    all_selections[strategy] = model.bandit.arm_selections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize arm selection frequencies\n",
    "visualize_arm_selection_frequencies(all_selections, models[strategies[0]].arms, max_time=num_steps-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cumulative reward over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "for strategy, data in results.items():\n",
    "    plt.plot(data['Cumulative Reward'], label=strategy)\n",
    "\n",
    "plt.title(f\"Cumulative Reward Over Time ({len(arm_params)} arms, {num_steps} steps)\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Cumulative Reward\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot average reward over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "for strategy, data in results.items():\n",
    "    average_reward = data['Cumulative Reward'] / (np.arange(len(data)) + 1)\n",
    "    plt.plot(average_reward, label=strategy)\n",
    "\n",
    "plt.title(f\"Average Reward Over Time ({len(arm_params)} arms, {num_steps} steps)\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Average Reward\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print final arm values and counts for each strategy\n",
    "for strategy, model in models.items():\n",
    "    print(f\"\\nFinal arm values and counts for {strategy} strategy:\")\n",
    "    bandit = model.schedule.agents[-1]\n",
    "    for arm in model.arms:\n",
    "        print(f\"Arm {arm.unique_id}: Final mean = {arm.get_mean():.2f}, \"\n",
    "              f\"Estimated value = {bandit.values[arm.unique_id]:.2f}, \"\n",
    "              f\"Pull count = {bandit.counts[arm.unique_id]}\")\n",
    "    print(f\"Total reward: {bandit.cumulative_reward:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
